# -*- coding: utf-8 -*-
"""langchain 경량화모델 실험.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KPtBNT4wNSO1tTqnZUCZ9-4R4Xuzdox2
"""

# !pip install langchain
# !pip install langchain-community
# !pip install faiss-cpu

import os
import re
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

import json
import faiss
import numpy as np
import os
import zipfile
from openai import OpenAI
from sentence_transformers import SentenceTransformer

from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage

# ✅ OpenAI API 설정
client = OpenAI(api_key ="")  
MODEL_ID = "gpt-3.5-turbo"

# ✅ ZIP 압축 해제 (가장 먼저 실행)
zip_path = "game_data.zip"
extract_path = "extracted"
if not os.path.exists(extract_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print("✅ game_data.zip 압축 해제 완료!")

# ✅ 메모리 객체
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ✅ 출력 후처리 함수
def clean_output(raw_output: str) -> str:
    return raw_output.strip()

# ✅ game.json 로딩
with open("game.json", "r", encoding="utf-8") as f:
    game_data = json.load(f)
game_names = [g["game_name"] for g in game_data]

# ✅ 게임 이름 입력
selected_game = input("🎯 어떤 게임의 룰을 보고 싶나요? 정확한 게임명을 입력해주세요:\n").strip()
if selected_game not in game_names:
    print(f"❌ '{selected_game}' 게임을 찾을 수 없습니다.")
    exit()


# ✅ 전체 룰 불러오기
game_info = next(g for g in game_data if g["game_name"] == selected_game)
game_rule_text = game_info['text']

# ✅ 벡터 경로 설정
base_path = os.path.join(extract_path, "game_data")  # 실제 뱅.faiss가 있는 경로
faiss_path = os.path.join(base_path, f"{selected_game}.faiss")
chunks_path = os.path.join(base_path, f"{selected_game}.json")

# ✅ 존재 확인
for path in [faiss_path, chunks_path]:
    if not os.path.exists(path):
        print(f"❌ '{selected_game}'에 대한 벡터 파일이 없습니다: {path}")
        exit()

# ✅ 룰 요약 먼저 출력
print("\n🧠 AI가 룰을 분석하여 설명 중...\n")
response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {"role": "system", "content": (
            "너는 보드게임 룰 전문 AI야. 반드시 아래 규칙을 따라야 해:\n"
            "- 사용자가 선택한 보드게임의 룰 전체를 보고, 그 게임의 룰을 알기 쉽게 설명해줘.\n"
            "- 핵심 개념, 목표, 진행 방식, 승리 조건을 요약해줘.\n"
            "- 설명은 간결하고 구조적으로 작성해."
        )},
        {"role": "user", "content": f"게임 이름: {selected_game}\n\n룰 전체:\n{game_rule_text}\n\n이 게임의 룰을 설명해주세요."}
    ],
    temperature=0.7,
    max_tokens=768
)
initial_explanation = response.choices[0].message.content
print(f"\n📘 게임 '{selected_game}'의 룰 설명:\n{clean_output(initial_explanation)}")

# ✅ 임베딩 및 벡터 DB 로딩
embed_model = SentenceTransformer("BAAI/bge-m3")
index = faiss.read_index(faiss_path)
with open(chunks_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# ✅ 메모리 연동된 질문 루프
def answer_loop_with_memory(selected_game, user_q):
    if user_q.lower() == "q":
        return "❌ 'q'는 종료 명령입니다."

    q_vec = embed_model.encode([user_q], normalize_embeddings=True)
    D, I = index.search(np.array(q_vec), k=3)
    retrieved_chunks = [chunks[i] for i in I[0]]

    context = "\n\n".join(retrieved_chunks)

    # 이전 대화 내용 불러오기
    message_history = memory.load_memory_variables({})["chat_history"]
    messages = [
        {"role": "system", "content": (
            "너는 보드게임 룰 전문 AI야. 반드시 아래 규칙을 따라야 해:\n"
            "- 사용자의 질문에 대해 아래 룰 설명(context)에 있는 내용만 기반해서 답변해.\n"
            "- 룰 설명에 없는 정보는 절대로 지어내거나 상상하지 마.\n"
            "- 사람 이름, 장소, 시간, 인원수 등을 추측하거나 새로 만들어내지 마.\n"
            "- 답할 수 없는 질문이면 '해당 정보는 룰에 명시되어 있지 않습니다.' 라고 말해."
        )}
    ]
    for m in message_history:
        if isinstance(m, HumanMessage):
            messages.append({"role": "user", "content": m.content})
        elif isinstance(m, AIMessage):
            messages.append({"role": "assistant", "content": m.content})

    # 최신 질문 추가
    messages.append({"role": "user", "content": f"""
아래는 '{selected_game}' 보드게임의 룰 설명 일부입니다:

{context}

이 룰을 바탕으로 다음 질문에 정확하고 구체적으로 답변해줘:

질문: {user_q}
"""})

    response = client.chat.completions.create(
        model=MODEL_ID,
        messages=messages,
        temperature=0.7,
        max_tokens=768
    )

    answer = response.choices[0].message.content
    memory.save_context({"input": user_q}, {"output": answer})
    return clean_output(answer)

# ✅ 콘솔 기반 질문 반복
while True:
    user_q = input("\n❓ 룰에 대해 궁금한 점을 질문하세요 (종료: q): ").strip()
    if user_q.lower() == "q":
        print("👋 챗봇을 종료합니다.")
        break
    answer = answer_loop_with_memory(selected_game, user_q)
    print(f"\n💬 답변:\n{answer}")

"""ㅜㅜㅜㅜ일단 뱅 으로만 pkl 만드는 코드"""

import os
import json
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

# ✅ 경로 설정
game_name = "뱅"
base_path = "extracted/game_data"
json_path = os.path.join(base_path, f"{game_name}.json")

# ✅ JSON 로딩
with open(json_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# ✅ 리스트가 아닌 dict 형식이면 변환 (예: {"data": [{"text": "..."}]})
if isinstance(chunks, dict) and "data" in chunks:
    chunks = [entry["text"] for entry in chunks["data"]]

# ✅ 빈 데이터 예외 처리
if not chunks:
    print(f"❌ {game_name} → 변환된 텍스트 없음")
else:
    # ✅ Document 객체로 변환
    docs = [Document(page_content=text) for text in chunks]

    # ✅ 임베딩 생성기 정의
    embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

    # ✅ 벡터 저장
    vectorstore = FAISS.from_documents(docs, embedding)
    vectorstore.save_local(base_path, index_name=game_name)
    print(f"✅ {game_name} 벡터 DB 저장 완료 (faiss + pkl)")

import os
import json
from openai import OpenAI
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer

# ✅ OpenAI 클라이언트 설정
client = OpenAI(api_key="")
MODEL_ID = "gpt-3.5-turbo"

# ✅ 대상 게임
game_name = "뱅"
base_path = "extracted/game_data"
faiss_path = os.path.join(base_path, f"{game_name}.faiss")
pkl_path = os.path.join(base_path, f"{game_name}.pkl")
json_path = os.path.join(base_path, f"{game_name}.json")

# ✅ 게임 룰 텍스트 로드
with open("game.json", "r", encoding="utf-8") as f:
    game_data = json.load(f)
game_info = next(g for g in game_data if g["game_name"] == game_name)
game_rule_text = game_info["text"]

# ✅ 벡터 DB 로드 (Pickle 허용)
embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
vectorstore = FAISS.load_local(
    folder_path=base_path,
    index_name=game_name,
    embeddings=embedding,
    allow_dangerous_deserialization=True
)

# ✅ FAISS에서 chunk 로드
with open(json_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# ✅ 메모리 객체
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ✅ 룰 요약 출력
summary_prompt = f"""
게임 이름: {game_name}
룰 전체:
{game_rule_text}

이 게임의 룰을 간단히 요약해서 설명해줘.
"""
print("\n🧠 게임 룰 요약:")
response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {"role": "system", "content": "너는 보드게임 룰을 요약하고 설명하는 AI야."},
        {"role": "user", "content": summary_prompt}
    ],
    temperature=0.7
)
summary = response.choices[0].message.content.strip()
print(f"\n📘 요약:\n{summary}")

# ✅ 질문 루프
embed_model = SentenceTransformer("BAAI/bge-m3")

def ask_with_memory(question):
    # 유사한 룰 청크 추출
    q_vec = embed_model.encode([question], normalize_embeddings=True)
    D, I = vectorstore.index.search(q_vec, k=3)
    retrieved_chunks = [chunks[i] for i in I[0]]
    context = "\n\n".join(retrieved_chunks)

    # 대화 이력 구성
    chat_history = memory.load_memory_variables({})["chat_history"]
    messages = [{"role": "system", "content": (
        "너는 보드게임 룰을 설명하는 AI야. 아래 context 기반으로만 정확히 답변해줘. "
        "룰에 없는 정보는 '룰에 명시되어 있지 않습니다'라고 답해."
    )}]
    for m in chat_history:
        if isinstance(m, HumanMessage):
            messages.append({"role": "user", "content": m.content})
        elif isinstance(m, AIMessage):
            messages.append({"role": "assistant", "content": m.content})

    # 현재 질문 추가
    messages.append({"role": "user", "content": f"""
다음은 '{game_name}'의 일부 룰 설명이야:

{context}

이 룰을 기반으로 아래 질문에 답해줘:
질문: {question}
"""})

    # 응답 생성
    response = client.chat.completions.create(
        model=MODEL_ID,
        messages=messages,
        temperature=0.7
    )
    answer = response.choices[0].message.content.strip()

    # 메모리에 저장
    memory.save_context({"input": question}, {"output": answer})
    return answer

# ✅ 콘솔 질문 루프
while True:
    q = input("\n❓ 궁금한 점을 입력하세요 (종료: q): ").strip()
    if q.lower() == "q":
        print("👋 챗봇을 종료합니다.")
        break
    a = ask_with_memory(q)
    print(f"\n💬 답변:\n{a}")

