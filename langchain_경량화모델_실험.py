# -*- coding: utf-8 -*-
"""langchain ê²½ëŸ‰í™”ëª¨ë¸ ì‹¤í—˜.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KPtBNT4wNSO1tTqnZUCZ9-4R4Xuzdox2
"""

# !pip install langchain
# !pip install langchain-community
# !pip install faiss-cpu

import os
import re
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

import json
import faiss
import numpy as np
import os
import zipfile
from openai import OpenAI
from sentence_transformers import SentenceTransformer

from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage

# âœ… OpenAI API ì„¤ì •
client = OpenAI(api_key ="")  
MODEL_ID = "gpt-3.5-turbo"

# âœ… ZIP ì••ì¶• í•´ì œ (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
zip_path = "game_data.zip"
extract_path = "extracted"
if not os.path.exists(extract_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print("âœ… game_data.zip ì••ì¶• í•´ì œ ì™„ë£Œ!")

# âœ… ë©”ëª¨ë¦¬ ê°ì²´
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# âœ… ì¶œë ¥ í›„ì²˜ë¦¬ í•¨ìˆ˜
def clean_output(raw_output: str) -> str:
    return raw_output.strip()

# âœ… game.json ë¡œë”©
with open("game.json", "r", encoding="utf-8") as f:
    game_data = json.load(f)
game_names = [g["game_name"] for g in game_data]

# âœ… ê²Œì„ ì´ë¦„ ì…ë ¥
selected_game = input("ğŸ¯ ì–´ë–¤ ê²Œì„ì˜ ë£°ì„ ë³´ê³  ì‹¶ë‚˜ìš”? ì •í™•í•œ ê²Œì„ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”:\n").strip()
if selected_game not in game_names:
    print(f"âŒ '{selected_game}' ê²Œì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()


# âœ… ì „ì²´ ë£° ë¶ˆëŸ¬ì˜¤ê¸°
game_info = next(g for g in game_data if g["game_name"] == selected_game)
game_rule_text = game_info['text']

# âœ… ë²¡í„° ê²½ë¡œ ì„¤ì •
base_path = os.path.join(extract_path, "game_data")  # ì‹¤ì œ ë±….faissê°€ ìˆëŠ” ê²½ë¡œ
faiss_path = os.path.join(base_path, f"{selected_game}.faiss")
chunks_path = os.path.join(base_path, f"{selected_game}.json")

# âœ… ì¡´ì¬ í™•ì¸
for path in [faiss_path, chunks_path]:
    if not os.path.exists(path):
        print(f"âŒ '{selected_game}'ì— ëŒ€í•œ ë²¡í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}")
        exit()

# âœ… ë£° ìš”ì•½ ë¨¼ì € ì¶œë ¥
print("\nğŸ§  AIê°€ ë£°ì„ ë¶„ì„í•˜ì—¬ ì„¤ëª… ì¤‘...\n")
response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {"role": "system", "content": (
            "ë„ˆëŠ” ë³´ë“œê²Œì„ ë£° ì „ë¬¸ AIì•¼. ë°˜ë“œì‹œ ì•„ë˜ ê·œì¹™ì„ ë”°ë¼ì•¼ í•´:\n"
            "- ì‚¬ìš©ìê°€ ì„ íƒí•œ ë³´ë“œê²Œì„ì˜ ë£° ì „ì²´ë¥¼ ë³´ê³ , ê·¸ ê²Œì„ì˜ ë£°ì„ ì•Œê¸° ì‰½ê²Œ ì„¤ëª…í•´ì¤˜.\n"
            "- í•µì‹¬ ê°œë…, ëª©í‘œ, ì§„í–‰ ë°©ì‹, ìŠ¹ë¦¬ ì¡°ê±´ì„ ìš”ì•½í•´ì¤˜.\n"
            "- ì„¤ëª…ì€ ê°„ê²°í•˜ê³  êµ¬ì¡°ì ìœ¼ë¡œ ì‘ì„±í•´."
        )},
        {"role": "user", "content": f"ê²Œì„ ì´ë¦„: {selected_game}\n\në£° ì „ì²´:\n{game_rule_text}\n\nì´ ê²Œì„ì˜ ë£°ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
    ],
    temperature=0.7,
    max_tokens=768
)
initial_explanation = response.choices[0].message.content
print(f"\nğŸ“˜ ê²Œì„ '{selected_game}'ì˜ ë£° ì„¤ëª…:\n{clean_output(initial_explanation)}")

# âœ… ì„ë² ë”© ë° ë²¡í„° DB ë¡œë”©
embed_model = SentenceTransformer("BAAI/bge-m3")
index = faiss.read_index(faiss_path)
with open(chunks_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# âœ… ë©”ëª¨ë¦¬ ì—°ë™ëœ ì§ˆë¬¸ ë£¨í”„
def answer_loop_with_memory(selected_game, user_q):
    if user_q.lower() == "q":
        return "âŒ 'q'ëŠ” ì¢…ë£Œ ëª…ë ¹ì…ë‹ˆë‹¤."

    q_vec = embed_model.encode([user_q], normalize_embeddings=True)
    D, I = index.search(np.array(q_vec), k=3)
    retrieved_chunks = [chunks[i] for i in I[0]]

    context = "\n\n".join(retrieved_chunks)

    # ì´ì „ ëŒ€í™” ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°
    message_history = memory.load_memory_variables({})["chat_history"]
    messages = [
        {"role": "system", "content": (
            "ë„ˆëŠ” ë³´ë“œê²Œì„ ë£° ì „ë¬¸ AIì•¼. ë°˜ë“œì‹œ ì•„ë˜ ê·œì¹™ì„ ë”°ë¼ì•¼ í•´:\n"
            "- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ë£° ì„¤ëª…(context)ì— ìˆëŠ” ë‚´ìš©ë§Œ ê¸°ë°˜í•´ì„œ ë‹µë³€í•´.\n"
            "- ë£° ì„¤ëª…ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ë¡œ ì§€ì–´ë‚´ê±°ë‚˜ ìƒìƒí•˜ì§€ ë§ˆ.\n"
            "- ì‚¬ëŒ ì´ë¦„, ì¥ì†Œ, ì‹œê°„, ì¸ì›ìˆ˜ ë“±ì„ ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒˆë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆ.\n"
            "- ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë©´ 'í•´ë‹¹ ì •ë³´ëŠ” ë£°ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.' ë¼ê³  ë§í•´."
        )}
    ]
    for m in message_history:
        if isinstance(m, HumanMessage):
            messages.append({"role": "user", "content": m.content})
        elif isinstance(m, AIMessage):
            messages.append({"role": "assistant", "content": m.content})

    # ìµœì‹  ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": f"""
ì•„ë˜ëŠ” '{selected_game}' ë³´ë“œê²Œì„ì˜ ë£° ì„¤ëª… ì¼ë¶€ì…ë‹ˆë‹¤:

{context}

ì´ ë£°ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì¤˜:

ì§ˆë¬¸: {user_q}
"""})

    response = client.chat.completions.create(
        model=MODEL_ID,
        messages=messages,
        temperature=0.7,
        max_tokens=768
    )

    answer = response.choices[0].message.content
    memory.save_context({"input": user_q}, {"output": answer})
    return clean_output(answer)

# âœ… ì½˜ì†” ê¸°ë°˜ ì§ˆë¬¸ ë°˜ë³µ
while True:
    user_q = input("\nâ“ ë£°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()
    if user_q.lower() == "q":
        print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    answer = answer_loop_with_memory(selected_game, user_q)
    print(f"\nğŸ’¬ ë‹µë³€:\n{answer}")

"""ã…œã…œã…œã…œì¼ë‹¨ ë±… ìœ¼ë¡œë§Œ pkl ë§Œë“œëŠ” ì½”ë“œ"""

import os
import json
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

# âœ… ê²½ë¡œ ì„¤ì •
game_name = "ë±…"
base_path = "extracted/game_data"
json_path = os.path.join(base_path, f"{game_name}.json")

# âœ… JSON ë¡œë”©
with open(json_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# âœ… ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ dict í˜•ì‹ì´ë©´ ë³€í™˜ (ì˜ˆ: {"data": [{"text": "..."}]})
if isinstance(chunks, dict) and "data" in chunks:
    chunks = [entry["text"] for entry in chunks["data"]]

# âœ… ë¹ˆ ë°ì´í„° ì˜ˆì™¸ ì²˜ë¦¬
if not chunks:
    print(f"âŒ {game_name} â†’ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ì—†ìŒ")
else:
    # âœ… Document ê°ì²´ë¡œ ë³€í™˜
    docs = [Document(page_content=text) for text in chunks]

    # âœ… ì„ë² ë”© ìƒì„±ê¸° ì •ì˜
    embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

    # âœ… ë²¡í„° ì €ì¥
    vectorstore = FAISS.from_documents(docs, embedding)
    vectorstore.save_local(base_path, index_name=game_name)
    print(f"âœ… {game_name} ë²¡í„° DB ì €ì¥ ì™„ë£Œ (faiss + pkl)")

import os
import json
from openai import OpenAI
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer

# âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(api_key="")
MODEL_ID = "gpt-3.5-turbo"

# âœ… ëŒ€ìƒ ê²Œì„
game_name = "ë±…"
base_path = "extracted/game_data"
faiss_path = os.path.join(base_path, f"{game_name}.faiss")
pkl_path = os.path.join(base_path, f"{game_name}.pkl")
json_path = os.path.join(base_path, f"{game_name}.json")

# âœ… ê²Œì„ ë£° í…ìŠ¤íŠ¸ ë¡œë“œ
with open("game.json", "r", encoding="utf-8") as f:
    game_data = json.load(f)
game_info = next(g for g in game_data if g["game_name"] == game_name)
game_rule_text = game_info["text"]

# âœ… ë²¡í„° DB ë¡œë“œ (Pickle í—ˆìš©)
embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
vectorstore = FAISS.load_local(
    folder_path=base_path,
    index_name=game_name,
    embeddings=embedding,
    allow_dangerous_deserialization=True
)

# âœ… FAISSì—ì„œ chunk ë¡œë“œ
with open(json_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# âœ… ë©”ëª¨ë¦¬ ê°ì²´
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# âœ… ë£° ìš”ì•½ ì¶œë ¥
summary_prompt = f"""
ê²Œì„ ì´ë¦„: {game_name}
ë£° ì „ì²´:
{game_rule_text}

ì´ ê²Œì„ì˜ ë£°ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì¤˜.
"""
print("\nğŸ§  ê²Œì„ ë£° ìš”ì•½:")
response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {"role": "system", "content": "ë„ˆëŠ” ë³´ë“œê²Œì„ ë£°ì„ ìš”ì•½í•˜ê³  ì„¤ëª…í•˜ëŠ” AIì•¼."},
        {"role": "user", "content": summary_prompt}
    ],
    temperature=0.7
)
summary = response.choices[0].message.content.strip()
print(f"\nğŸ“˜ ìš”ì•½:\n{summary}")

# âœ… ì§ˆë¬¸ ë£¨í”„
embed_model = SentenceTransformer("BAAI/bge-m3")

def ask_with_memory(question):
    # ìœ ì‚¬í•œ ë£° ì²­í¬ ì¶”ì¶œ
    q_vec = embed_model.encode([question], normalize_embeddings=True)
    D, I = vectorstore.index.search(q_vec, k=3)
    retrieved_chunks = [chunks[i] for i in I[0]]
    context = "\n\n".join(retrieved_chunks)

    # ëŒ€í™” ì´ë ¥ êµ¬ì„±
    chat_history = memory.load_memory_variables({})["chat_history"]
    messages = [{"role": "system", "content": (
        "ë„ˆëŠ” ë³´ë“œê²Œì„ ë£°ì„ ì„¤ëª…í•˜ëŠ” AIì•¼. ì•„ë˜ context ê¸°ë°˜ìœ¼ë¡œë§Œ ì •í™•íˆ ë‹µë³€í•´ì¤˜. "
        "ë£°ì— ì—†ëŠ” ì •ë³´ëŠ” 'ë£°ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´."
    )}]
    for m in chat_history:
        if isinstance(m, HumanMessage):
            messages.append({"role": "user", "content": m.content})
        elif isinstance(m, AIMessage):
            messages.append({"role": "assistant", "content": m.content})

    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": f"""
ë‹¤ìŒì€ '{game_name}'ì˜ ì¼ë¶€ ë£° ì„¤ëª…ì´ì•¼:

{context}

ì´ ë£°ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ ì§ˆë¬¸ì— ë‹µí•´ì¤˜:
ì§ˆë¬¸: {question}
"""})

    # ì‘ë‹µ ìƒì„±
    response = client.chat.completions.create(
        model=MODEL_ID,
        messages=messages,
        temperature=0.7
    )
    answer = response.choices[0].message.content.strip()

    # ë©”ëª¨ë¦¬ì— ì €ì¥
    memory.save_context({"input": question}, {"output": answer})
    return answer

# âœ… ì½˜ì†” ì§ˆë¬¸ ë£¨í”„
while True:
    q = input("\nâ“ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()
    if q.lower() == "q":
        print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    a = ask_with_memory(q)
    print(f"\nğŸ’¬ ë‹µë³€:\n{a}")

