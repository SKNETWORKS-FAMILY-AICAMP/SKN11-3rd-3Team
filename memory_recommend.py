# -*- coding: utf-8 -*-
"""memory_recommend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yYiCtsvL4-x1Cotwebg61IDq8vBgn1Ij
"""

# !pip install langchain
# !pip install openai faiss-cpu sentence-transformers langchain-core
# !pip install -U langchain langchain-openai

import os
import json
import re
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI




# âœ… 1. ì„¸ì…˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ í´ë˜ìŠ¤ ì •ì˜
class InMemoryHistory(BaseChatMessageHistory):
    def __init__(self):
        self.messages = []

    def add_messages(self, messages):
        self.messages.extend(messages)

    def clear(self):
        self.messages = []

    def __repr__(self):
        return str(self.messages)


store = {}
def get_by_session_id(session_id):
    if session_id not in store:
        store[session_id] = InMemoryHistory()
    return store[session_id]


# âœ… 2. FAISS + ê²Œì„ ë°ì´í„° ë¡œë“œ
index = faiss.read_index("game_index.faiss")

with open("texts.json", "r", encoding="utf-8") as f:
    texts = json.load(f)

with open("game_names.json", "r", encoding="utf-8") as f:
    game_names = json.load(f)

embed_model = SentenceTransformer("BAAI/bge-m3", device="cpu")


# âœ… 3. LangChain Prompt + LLM + Chain ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        "ë„ˆëŠ” ë³´ë“œê²Œì„ ì¶”ì²œ ë„ìš°ë¯¸ì•¼. ë‹¤ìŒì€ ì¶”ì²œ ê°€ëŠ¥í•œ ê²Œì„ ì„¤ëª…ë“¤ì´ì•¼:\n\n{context}\n\n"
        "ë°˜ë“œì‹œ ì´ ê²Œì„ ëª©ë¡ ì•ˆì—ì„œë§Œ ì¶”ì²œí•´. ìƒˆë¡œìš´ ê²Œì„ì„ ì§€ì–´ë‚´ì§€ ë§ˆ.\n"
        "ì§ˆë¬¸ì— ë§ëŠ” ê²Œì„ 3ê°œë¥¼ ê³¨ë¼ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µí•´:\n"
        "ê²Œì„ëª…1: ì´ìœ \nê²Œì„ëª…2: ì´ìœ \nê²Œì„ëª…3: ì´ìœ \në§ˆì§€ë§‰ ì¤„ì€ 'ì¶”ì²œ ì™„ë£Œ!'ë¡œ ëë‚´."
    ),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{query}")
])

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.7,  openai_api_key="")

chain = prompt | llm
chain_with_memory = RunnableWithMessageHistory(
    chain,
    get_session_history=get_by_session_id,
    input_messages_key="query",
    history_messages_key="history"
)


# âœ… 4. ìœ ì‚¬ ê²Œì„ ì„¤ëª… ê²€ìƒ‰ í•¨ìˆ˜
def search_similar_context(query, top_k=3):
    query_vec = embed_model.encode([query], normalize_embeddings=True)
    D, I = index.search(np.array(query_vec), top_k)

    context_blocks = []
    for i in I[0]:
        context_blocks.append(f"[{game_names[i]}]\n{texts[i]}")
    return "\n\n".join(context_blocks)


# âœ… 5. ì‹¤í–‰ ë£¨í”„
session_id = "user1"

while True:
    user_query = input("ğŸ® ì›í•˜ëŠ” ê²Œì„ ìŠ¤íƒ€ì¼ì„ ì…ë ¥í•˜ì„¸ìš” ('q' ì¢…ë£Œ): ")
    if user_query.lower() == "q":
        print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    context = search_similar_context(user_query)
    response = chain_with_memory.invoke(
        {"query": user_query, "context": context},
        config={"configurable": {"session_id": session_id}}
    )

    print("\nğŸ¤– ì¶”ì²œ ê²°ê³¼:\n")
    print(response.content)